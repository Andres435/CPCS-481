# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x5f9v_wvssBSlBCS5zNIYsNEVG7BVfrH

# Sentiment Analysis
"""

from google.colab import drive
drive.mount('/content/drive')
import os
#Changing the directory
os.chdir('/content/drive/My Drive/CPCS 481')

import numpy as np
import pandas as pd
import nltk

"""## Import Dataset"""

dataset = pd.read_csv('Amazon_Food_Review.csv')
dataset['Combined_Text'] = dataset['Summary'] + ' ' + dataset['Text']

print(dataset['Combined_Text'])

"""### Preprocessing"""

from nltk.tokenize import RegexpTokenizer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords

nltk.download('stopwords')
nltk.download('wordnet')

"""### Tokenize"""

tokenizer = RegexpTokenizer(r'\w+')
en_stopwords = set(stopwords.words('english'))

"""### Clean Data"""

def preprocess_text(text):
    # Convert to lowercase
    if isinstance(text, str):
      text = text.lower()

      # Tokenization
      tokens = tokenizer.tokenize(text)

      # Remove stopwords
      tokens = [token for token in tokens if token.isalnum() and token not in en_stopwords]

      # Stemming
      ps = PorterStemmer()
      tokens = [ps.stem(token) for token in tokens]

      wnet =  WordNetLemmatizer()
      lem = []
      for words in tokens:
          w = []
          for token in words:
              w.append(wnet.lemmatize(token))
          lem.append(w)

      return ' '.join(tokens)
    else:
        # If not a string, return an empty string
        return ""

# Process 'Summary' and 'Text' columns separately
dataset['Processed_Text'] = dataset['Combined_Text'].apply(preprocess_text)

print(dataset['Processed_Text'])

"""## Naive Bayers"""

dataset['Sentiment'] = np.where(dataset['Score'] > 3, 'Positive', np.where(dataset['Score'] < 3, 'Negative', 'Neutral'))

"""### TFIDF Vectorization"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer()
X = tfidf_vectorizer.fit_transform(dataset['Processed_Text']).toarray()
y = dataset['Sentiment'].values

"""### Training the Naive Bayes model on the Training set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

print(X_train)

print(X_test)

print(y_train)

print(y_test)

from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(X_train, y_train)

"""### Predicting the Test set results"""

y_pred = classifier.predict(X_test)
print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)) , 1))

"""## Making the Confusion Matrix"""

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)